{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(ABC):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        '''You can change this for your player if you need to handle state/have memory'''\n",
    "        self.name = name\n",
    "        pass\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'{self.name}'\n",
    "\n",
    "    def is_agent(self) -> bool:\n",
    "        return isinstance(self, MyAgent)\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_move(self, game: 'Game') -> tuple[int, int]:\n",
    "        '''\n",
    "        game: the Quixo game. You can use it to override the current game with yours, but everything is evaluated by the main game\n",
    "        return values: this method shall return a tuple of X,Y positions and a move among TOP, BOTTOM, LEFT and RIGHT\n",
    "        '''\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(object):\n",
    "    def __init__(self) -> None:\n",
    "        self.winner = None\n",
    "        self._current_player_idx = 1\n",
    "        self._board = np.ones((3, 3), dtype=np.uint8) * -1\n",
    "        self._available_moves_list = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
    "        self._emojis = ['❌', '⭕️', '⚪️']\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        original_stdout = sys.stdout\n",
    "        output_buffer = io.StringIO()\n",
    "        sys.stdout = output_buffer\n",
    "\n",
    "        for r, row in enumerate(self._board):\n",
    "            for t, tile in enumerate(row):\n",
    "                print(self._emojis[tile] , end=' ')\n",
    "            print()\n",
    "\n",
    "        sys.stdout = original_stdout\n",
    "        captured_output = output_buffer.getvalue()\n",
    "        return captured_output\n",
    "\n",
    "    def check_winner(self) -> int:\n",
    "        for x in range(self._board.shape[0]):\n",
    "            if all(self._board[x, :] == self._board[x, 0]):\n",
    "                return self._board[x, 0]\n",
    "        for y in range(self._board.shape[0]):\n",
    "            if all(self._board[:, y] == self._board[0, y]):\n",
    "                return self._board[0, y]\n",
    "        if all([self._board[x, x] for x in range(self._board.shape[0])] == self._board[0, 0]):\n",
    "            return self._board[0, 0]\n",
    "        if all([self._board[x, -x] for x in range(self._board.shape[0])] == self._board[-1, -1]):\n",
    "            return self._board[0, -1]\n",
    "\n",
    "        # tie\n",
    "        if len(self._available_moves_list) == 0:\n",
    "            return 2\n",
    "\n",
    "        return -1\n",
    "\n",
    "    def play(self, player1: Player, player2: Player) -> int:\n",
    "        '''Play the game. Returns the winning player'''\n",
    "        players = [player1, player2]\n",
    "        winner = -1\n",
    "        while winner < 0:\n",
    "            self._current_player_idx += 1\n",
    "            self._current_player_idx %= len(players)\n",
    "            ok = False\n",
    "            while not ok:\n",
    "                move = players[self._current_player_idx].make_move(self)\n",
    "                ok = (self._board[move] == -1)\n",
    "                if ok:\n",
    "                    self._board[move] = self._current_player_idx\n",
    "                elif isinstance(players[self._current_player_idx], HumanPlayer):\n",
    "                    print(\"That's an invalid move, please reenter your move:\")\n",
    "            self._available_moves_list.remove(move)\n",
    "            # print(self)\n",
    "            winner = self.check_winner()\n",
    "        \n",
    "        if winner > 1:\n",
    "            self.winner = None\n",
    "        else:\n",
    "            self.winner = players[winner]\n",
    "            \n",
    "        return winner\n",
    "\n",
    "    def single_move(self, move: tuple[int, int]) -> None:\n",
    "        self._board[move] = self._current_player_idx\n",
    "\n",
    "    def get_available_moves(self) -> list[tuple[int, int]]:\n",
    "        '''return the possible moves in the current position'''\n",
    "        return self._available_moves_list\n",
    "\n",
    "    def get_hash(self) -> str:\n",
    "        '''hashes the state of the board'''\n",
    "        return str(self._board.reshape(3 * 3))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Players Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer(Player):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "\n",
    "    def make_move(self, game: 'Game') -> tuple[int, int]:\n",
    "        move = (random.randint(0, 2), random.randint(0, 2))\n",
    "        return move\n",
    "\n",
    "class MyAgent(Player):\n",
    "    def __init__(self, name: str, exp_rate=0.3) -> None:\n",
    "        super().__init__(name)\n",
    "        self._states = list()\n",
    "        self._state_value = dict()\n",
    "        self._lr = 0.2\n",
    "        self._exp_rate = exp_rate\n",
    "        self._decay_gamma = 0.9\n",
    "\n",
    "    def make_move(self, game: Game) -> tuple[int, int]:\n",
    "        move = self.__choose_action(game)\n",
    "        return move\n",
    "\n",
    "    def feed_reward(self, reward: float) -> None:\n",
    "        for st in reversed(self._states):\n",
    "            if self._state_value.get(st) is None:\n",
    "                self._state_value[st] = 0\n",
    "            self._state_value[st] += self._lr * (self._decay_gamma * reward - self._state_value[st])\n",
    "            reward = self._state_value[st]\n",
    "\n",
    "    def __choose_action(self, game: Game) -> tuple[int, int]:\n",
    "        possible_moves = game.get_available_moves()\n",
    "        if np.random.uniform(0, 1) <= self._exp_rate:\n",
    "            # take random action\n",
    "            idx = np.random.choice(len(possible_moves))\n",
    "            action = possible_moves[idx]\n",
    "        else:\n",
    "            value_max = -999\n",
    "            for pm in possible_moves:\n",
    "                next_state = deepcopy(game)\n",
    "                next_state.single_move(pm)\n",
    "                next_hash = next_state.get_hash()\n",
    "                value = 0 if self._state_value.get(next_hash) is None else self._state_value.get(next_hash)\n",
    "                if value >= value_max:\n",
    "                    value_max = value\n",
    "                    action = pm\n",
    "\n",
    "        next_state = deepcopy(game)\n",
    "        next_state.single_move(action)\n",
    "        next_hash = next_state.get_hash()\n",
    "        self._states.append(next_hash)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def reset_states(self) -> None:\n",
    "        self._states.clear()\n",
    "\n",
    "    def save_policy(self) -> None:\n",
    "        fw = open('policy_' + str(self.name), 'wb')\n",
    "        pickle.dump(self._state_value, fw)\n",
    "        fw.close()\n",
    "\n",
    "    def load_policy(self, file) -> None:\n",
    "        fr = open(file, 'rb')\n",
    "        self._state_value = pickle.load(fr)\n",
    "        fr.close()\n",
    "\n",
    "    def set_exp_rate(self, exp_rate: float=0.3) -> None:\n",
    "        self._exp_rate = exp_rate\n",
    "\n",
    "class HumanPlayer(Player):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "\n",
    "    def make_move(self, game: Game) -> tuple[int, int]:\n",
    "        x = int(input(\"Input the x coordinate (from 0 to 2):\"))\n",
    "        y = int(input(\"Input the y coordinate (from 0 to 2):\"))\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, player1: int, player2: int, name1: str='player1', name2: str='player2', policy1: str=None, policy2: str=None, testing: bool=False) -> None:\n",
    "        self._players_map = {'RandomPlayer': RandomPlayer, 'HumanPlayer': HumanPlayer, 'MyAgent': MyAgent}\n",
    "        self._player1 = self._players_map.get(player1)(name=name1)\n",
    "        self._player2 = self._players_map.get(player2)(name=name2)\n",
    "        \n",
    "        if policy1 is not None:\n",
    "            if self._player1.is_agent():\n",
    "                self._player1.load_policy(policy1)\n",
    "            else:\n",
    "                print(f\"WARNING: policy1 was not loaded to player1, since it's not an instance of MyAgent.\")\n",
    "            \n",
    "        if policy2 is not None:\n",
    "            if self._player2.is_agent():\n",
    "                self._player2.load_policy(policy2)\n",
    "            else:\n",
    "                print(f\"WARNING: policy2 was not loaded to player2, since it's not an instance of MyAgent.\")\n",
    "\n",
    "    def training(self, rounds=1000) -> None:\n",
    "        if not (self._player1.is_agent() or self._player2.is_agent()):\n",
    "            print(\"ERROR: cannot start training with no agents.\")\n",
    "            return\n",
    "\n",
    "        if self._player1.name == self._player2.name:\n",
    "            self._player2.name = \"player2\"   \n",
    "\n",
    "        for i in tqdm(range(rounds)):\n",
    "            game = Game()\n",
    "            winner = game.play(self._player1, self._player2)\n",
    "\n",
    "            if winner == 0:\n",
    "                if self._player1.is_agent():\n",
    "                    self._player1.feed_reward(1)\n",
    "                    self._player1.reset_states()\n",
    "                if self._player2.is_agent():\n",
    "                    self._player2.feed_reward(0)\n",
    "                    self._player2.reset_states()\n",
    "            elif winner == 1:\n",
    "                if self._player1.is_agent():\n",
    "                    self._player1.feed_reward(0)\n",
    "                    self._player1.reset_states()\n",
    "                if self._player2.is_agent():\n",
    "                    self._player2.feed_reward(1)\n",
    "                    self._player2.reset_states()\n",
    "            elif winner == 2:\n",
    "                if self._player1.is_agent():\n",
    "                    self._player1.feed_reward(0.1)\n",
    "                    self._player1.reset_states()\n",
    "                if self._player2.is_agent():\n",
    "                    self._player2.feed_reward(0.5)\n",
    "                    self._player2.reset_states()\n",
    "                \n",
    "        if self._player1.is_agent():\n",
    "            self._player1.save_policy()\n",
    "        if self._player2.is_agent():\n",
    "            self._player2.save_policy()\n",
    "\n",
    "    def testing(self, rounds=1000) -> None:\n",
    "        wins = [0, 0]\n",
    "\n",
    "        if self._player1.is_agent():\n",
    "            self._player1.set_exp_rate(0)\n",
    "\n",
    "        if self._player2.is_agent():\n",
    "            self._player2.set_exp_rate(0)\n",
    "\n",
    "        for i in tqdm(range(rounds)):\n",
    "            game = Game()\n",
    "            winner = game.play(self._player1, self._player2)\n",
    "            if game.winner is not None:\n",
    "                wins[0] += (1 - winner)\n",
    "                wins[1] += winner\n",
    "\n",
    "        win_rate_p1 = (wins[0]/rounds)*100\n",
    "        draws = ((rounds - sum(wins))/rounds)*100\n",
    "\n",
    "        print(f\"The results of the match [{type(self._player1)} vs {type(self._player2)}] are shown here:\")\n",
    "        print(f\"The win rate for the player1 is {win_rate_p1:.2f}% on a total of {rounds} matches\")\n",
    "        print(f\"Thw two players drew {draws:.2f}% of the games\")\n",
    "\n",
    "    def single_match(self) -> None:        \n",
    "        game = Game()\n",
    "        game.play(self._player1, self._player2)\n",
    "\n",
    "        if game.winner is None:\n",
    "            print(\"The game ended in a draw\")\n",
    "        else:\n",
    "            print(f\"{game.winner} has won!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:10<00:00, 497.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of the match [<class '__main__.MyAgent'> vs <class '__main__.RandomPlayer'>] are shown here:\n",
      "The win rate for the player1 is 97.26% on a total of 5000 matches\n",
      "Thw two players drew 2.74% of the games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model = Model(player1='MyAgent', player2='MyAgent', policy1='policy_100kgames_p1', policy2='policy_100kgames_p2', name1='200kgames_p1', name2='200kgames_p2')\n",
    "# model.training(rounds=100000)\n",
    "model = Model(player1='MyAgent', player2='RandomPlayer', policy1='policy_200kgames_p1')\n",
    "model.testing(rounds=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Match with Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model(player1='HumanPlayer', player2='RandomPlayer')\n",
    "# model.single_match()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
